'''
This demo shows how to use the pre-trained Junas model to extract high-level embeddings from the auxiliary images and language descriptions.
The resulting embeddings serve as the prior distribution to guide MRI reconstruction
'''

import os
import torch
import torch.nn as nn
from tqdm import tqdm
import numpy as np
import h5py as h5
import sys
import shutil
from transformers import AutoModelForCausalLM
from Janus.janus.models import MultiModalityCausalLM, VLChatProcessor
from Janus.janus.utils.io import load_pil_images
from scipy.io import savemat,loadmat
import matplotlib.pyplot as plt
from einops import rearrange
import umap

def VecNormalization(vector):
    norm = np.linalg.norm(vector, axis=-1, keepdims=True)
    vector_norm = vector / (norm + 1e-8)
    return vector_norm

torch.manual_seed(3976)
torch.cuda.manual_seed(3976)
DEVICE     = torch.device('cuda:1' if torch.cuda.is_available() else 'cpu')

outpath =  './prior_embeddings_image_language/blurred_vs_aliasing'   

if not os.path.exists(outpath):
    os.makedirs(outpath)

try:
    current_file = os.path.abspath(sys.argv[0])
    shutil.copy2(current_file, outpath)
    print(f"file saved: {outpath}")
except Exception as e:
    print(f"save filed: {str(e)}")


'''load example images'''
img_exmaples = loadmat('./image_examples/blurred_and_aliased_images.mat')
PosImg  = img_exmaples['positive_img'].transpose(2,0,1)
NegImg  = img_exmaples['negative_img'].transpose(2,0,1)

PosImg  = torch.tensor(PosImg).float().to(DEVICE)
NegImg  = torch.tensor(NegImg).float().to(DEVICE)

'''language prompt'''
##"high-quality, low-quality" pair or "blurred, aliasing" pair
prompt = 'Determine whether this image is blurred or aliasing. Respond with single word.'  

'''load the pre-trained Junas model for embedding extraction'''
# this is the weight path of Junas model, which can be downloaded from here: https://huggingface.co/deepseek-ai/Janus-Pro-1B/tree/main
foundation_model_path = "/local_mount/space/cookie2/1/users/rf552/code/foundation_model_self_supervise/Janus_weights/Janus-Pro-1B"  # change to your folder
vl_chat_processor     = VLChatProcessor.from_pretrained(foundation_model_path)
vl_gpt                = AutoModelForCausalLM.from_pretrained(foundation_model_path, trust_remote_code=True)
vl_gpt                = vl_gpt.to(torch.bfloat16).to(DEVICE).eval()
for param in vl_gpt.parameters():
    param.requires_grad = False
tokenizer             = vl_chat_processor.tokenizer

# "./image_example/img.jpg" is just a placeholder example, included so we can
# leverage Janus's built-in pipeline for image preprocessing and formatting.
conversation = [
            {
                "role": "User",
                "content": f"<image_placeholder>\n{prompt}",
                "images": [f"./image_examples/img.jpg"],
            },
            {"role": "Assistant", "content": ""},
    ]

pil_images = load_pil_images(conversation)
prepare_inputs = vl_chat_processor(
            conversations=conversation, images=pil_images, force_batchify=True
        ).to(vl_gpt.device)  
inputs_embeds = vl_gpt.prepare_inputs_embeds(**prepare_inputs)
images_emb_mask  = rearrange(prepare_inputs.images_emb_mask, "b n t -> b (n t)")
# Sequence-level mask indicating where image sequences appear in the conversation.
# Each position is 1 if that part of the sequence comes from an image, else 0.
images_seq_mask = (prepare_inputs.images_seq_mask)
images_seq_mask2 = (prepare_inputs.images_seq_mask).unsqueeze(-1).float()

'''Calculate calculate image-language embeddings'''
pos_features = []
neg_features = []
for sli in range(NegImg.shape[0]):
    neg_img    = NegImg[sli]
    # use the image encoder to extract the image embedding
    neg_vision_features =  vl_gpt.aligner(vl_gpt.vision_model(neg_img.to(torch.bfloat16).unsqueeze(0).unsqueeze(1).repeat(1,3,1,1))) 
    ## Create a detached copy of the input embeddings for constructing negative samples.
    neg_inputs_embeds   = inputs_embeds.detach().clone()
    # Replace the image-related embedding positions with negative image features.
    # 'images_seq_mask' selects the positions in the sequence corresponding to images,
    # and 'images_emb_mask' selects the flattened visual-token positions.
    neg_inputs_embeds[images_seq_mask] = neg_vision_features[images_emb_mask]  # concatenation of image and language embeddings
        
    with torch.no_grad():
        outputs = vl_gpt.language_model(
                inputs_embeds=neg_inputs_embeds,
                attention_mask=prepare_inputs.attention_mask,
                output_hidden_states=True
        )
        neg_features.append(outputs.hidden_states[-1][:,-1,:].float().cpu().detach().numpy())    # the last token generated by the last layer of the large language transformer

for sli in range(PosImg.shape[0]):
    pos_img    = PosImg[sli]
    pos_vision_features    =  vl_gpt.aligner(vl_gpt.vision_model(pos_img.to(torch.bfloat16).unsqueeze(0).unsqueeze(1).repeat(1,3,1,1))) 
    pos_inputs_embeds    = inputs_embeds.detach().clone()
    pos_inputs_embeds[images_seq_mask] = pos_vision_features[images_emb_mask]

    with torch.no_grad():
        outputs = vl_gpt.language_model(
                inputs_embeds=pos_inputs_embeds,
                attention_mask=prepare_inputs.attention_mask,
                output_hidden_states=True
        )
        pos_features.append(outputs.hidden_states[-1][:,-1,:].float().cpu().detach().numpy())

neg_features = np.vstack(neg_features)
pos_features = np.vstack(pos_features)


with h5.File(os.path.join(outpath, 'features_real.h5'), 'w') as f:
    f.create_dataset('neg_features', data=neg_features)
    f.create_dataset('pos_features', data=pos_features)
  

'''Use Gaussian noise perturbation to augment the image-language embeddings'''
answer_list_ds = []
neg_features_aug = []
pos_features_aug = []
NumSample = 100

for nn in range(NumSample):
    print(nn)

    for sli in range(NegImg.shape[0]):
        noise_embeds = torch.randn_like(inputs_embeds)*0.02
        neg_img    = NegImg[sli]
        neg_vision_features =  vl_gpt.aligner(vl_gpt.vision_model(neg_img.to(torch.bfloat16).unsqueeze(0).unsqueeze(1).repeat(1,3,1,1))) 
        neg_inputs_embeds   = inputs_embeds.detach().clone()
        neg_inputs_embeds[images_seq_mask] = neg_vision_features[images_emb_mask]
        
        neg_inputs_embeds_aug = neg_inputs_embeds + noise_embeds*(1.-images_seq_mask2).to(torch.bfloat16)

        with torch.no_grad():
            outputs = vl_gpt.language_model(
                inputs_embeds=neg_inputs_embeds_aug,
                attention_mask=prepare_inputs.attention_mask,
                output_hidden_states=True
            )
            neg_features_aug.append(outputs.hidden_states[-1][:,-1,:].float().cpu().detach().numpy())

           
    for sli in range(PosImg.shape[0]):
        noise_embeds = torch.randn_like(inputs_embeds)*0.02
        pos_img    = PosImg[sli]
        pos_vision_features    =  vl_gpt.aligner(vl_gpt.vision_model(pos_img.to(torch.bfloat16).unsqueeze(0).unsqueeze(1).repeat(1,3,1,1))) 
        pos_inputs_embeds    = inputs_embeds.detach().clone()
        pos_inputs_embeds[images_seq_mask] = pos_vision_features[images_emb_mask]

        fully_inputs_embeds_aug = pos_inputs_embeds + noise_embeds*(1.-images_seq_mask2).to(torch.bfloat16)


        with torch.no_grad():
            outputs = vl_gpt.language_model(
                inputs_embeds=fully_inputs_embeds_aug,
                attention_mask=prepare_inputs.attention_mask,
                output_hidden_states=True
            )
            pos_features_aug.append(outputs.hidden_states[-1][:,-1,:].float().cpu().detach().numpy())


neg_features_aug = np.vstack(neg_features_aug)
pos_features_aug = np.vstack(pos_features_aug)

with h5.File(os.path.join(outpath, 'image_language_feat.h5'), 'w') as f:
    f.create_dataset('negative_features', data=neg_features_aug)
    f.create_dataset('positive_features', data=pos_features_aug)


'''UMAP visualization'''
neg_features = VecNormalization(neg_features)
pos_features = VecNormalization(pos_features)
neg_features_aug = VecNormalization(neg_features_aug[::2]) # for visualization
pos_features_aug = VecNormalization(pos_features_aug[::2])


all_features = np.vstack([neg_features, neg_features_aug, pos_features, pos_features_aug])

labels = np.array([0]*len(neg_features) + [1]*len(neg_features_aug) + [2]*len(pos_features) + [3]*len(pos_features_aug))  # 0: ????1: ??? 

umap_model = umap.UMAP(n_neighbors=30, min_dist=0.1, random_state=42)
features_2d = umap_model.fit_transform(all_features)

DS_2d_0        = features_2d[labels==0]
DS_2d_1        = features_2d[labels==1]
fully_2d_0     = features_2d[labels==2]
fully_2d_1     = features_2d[labels==3]


plt.rcParams.update({
    "font.size": 18,       
    "axes.titlesize": 18, 
    "axes.labelsize": 18,  
    "xtick.labelsize": 18, 
    "ytick.labelsize": 18, 
    "legend.fontsize": 18, 
})

fig, ax = plt.subplots()

ax.scatter(DS_2d_0[:, 0], DS_2d_0[:, 1], color='black', alpha=1, s=20,label='negative embeddings (real)',zorder=10)
ax.scatter(DS_2d_1[:, 0], DS_2d_1[:, 1], color='blue', alpha=0.3, s=20,label='negative embeddings (augmented)')

ax.scatter(fully_2d_0[:, 0], fully_2d_0[:, 1], color='red', alpha=1, s=20,label='positive embeddings (real)',zorder=10)
ax.scatter(fully_2d_1[:, 0], fully_2d_1[:, 1], color='orange', alpha=0.3, s=20, label='positive embeddings (augmented)')

ax.set_xlim(-2, 15)   # change the range for better visualization
ax.set_ylim(-2, 15)
ax.legend(
    #loc='center left',
    #bbox_to_anchor=(1, 0.5)
    fontsize=10
)
ax.set_title("UMAP Visualization")
ax.set_xlabel("Dim 1", labelpad=10)
ax.set_ylabel("Dim 2", labelpad=10)
plt.tight_layout()
plt.savefig(
    os.path.join(outpath,
                 "UMAP_visualization.png"),
    dpi=300,
    bbox_inches='tight'
)
plt.show()




